{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize_sentiment(tweet):\n",
    "    analysis = TextBlob((str(tweet)))\n",
    "    return analysis.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the path for the CSV and put it in here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('/Users/vanessahoang/Documents/DeepStockNLP/Data/18-20-csv/2018-2020_model_input.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the train news and test news datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_news(day, month, year):\n",
    "    index = 0\n",
    "    dataset = []\n",
    "    d1 = datetime(year, month, day).date() \n",
    "    for date in news['Date']:\n",
    "        d = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        if d1 > d:\n",
    "            dataset.append(news.iloc[index])\n",
    "        index = index +1\n",
    "        df = pd.DataFrame(dataset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_news(day, month, year):\n",
    "    index = 0\n",
    "    dataset = []\n",
    "    d1 = datetime(year, month, day).date() \n",
    "    for date in news['Date']:\n",
    "        d = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        if d1 < d:\n",
    "            dataset.append(news.iloc[index])\n",
    "        index = index +1\n",
    "        df = pd.DataFrame(dataset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_column = (len(news.columns))\n",
    "# day, month, year\n",
    "train_news = get_train_news(15, 1, 2020)\n",
    "test_news = get_test_news(14, 1, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news_list = []\n",
    "for row in range (0, len(train_news.index)):\n",
    "    train_news_list.append(' '.join(str(k) for k in train_news.iloc[row,12:num_column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = CountVectorizer(min_df=0.01, max_df=0.8)\n",
    "news_vector = vectorize.fit_transform(train_news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TABLE OF FREQUENCY WORD DISTRIBUTION (511, 46)\n"
     ]
    }
   ],
   "source": [
    "print(\"THE TABLE OF FREQUENCY WORD DISTRIBUTION\", news_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model: Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "model = lr.fit(news_vector, train_news[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = vectorize.transform(test_news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0    1\n",
       "Actual            \n",
       "0          14   83\n",
       "1          14  110"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(test_vector)\n",
    "pd.crosstab(test_news[\"Label\"], predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the baseline model accuracy 0.5610859728506787\n"
     ]
    }
   ],
   "source": [
    "accuracy1=accuracy_score(test_news[\"Label\"], predictions)\n",
    "print(\"the baseline model accuracy\", accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten words according to the baseline model         Word  Coefficient\n",
      "39       why     1.023201\n",
      "10  children     0.666932\n",
      "2         an     0.641347\n",
      "42   workers     0.532837\n",
      "17      from     0.514270\n",
      "5         as     0.504116\n",
      "1     amazon     0.460666\n",
      "11     china     0.448186\n",
      "33      that     0.422779\n",
      "22        it     0.397774\n",
      "Last ten words according to the baseline model         Word  Coefficient\n",
      "24       new    -0.451398\n",
      "6         be    -0.454282\n",
      "0      after    -0.514281\n",
      "44       you    -0.690286\n",
      "3        and    -0.764320\n",
      "15  facebook    -0.822030\n",
      "13  dealbook    -0.825163\n",
      "38        up    -0.832346\n",
      "23       its    -0.886823\n",
      "43      york    -1.023077\n"
     ]
    }
   ],
   "source": [
    "words = vectorize.get_feature_names()\n",
    "coefficients = model.coef_.tolist()[0]\n",
    "coeffdf = pd.DataFrame({'Word' : words, 'Coefficient' : coefficients})\n",
    "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0,1])\n",
    "print(\"Top ten words according to the baseline model\", coeffdf.head(10))\n",
    "print(\"Last ten words according to the baseline model\", coeffdf.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest with tfid and bigram 0.5656108597285068\n"
     ]
    }
   ],
   "source": [
    "nvectorize = TfidfVectorizer(min_df=0.01, max_df=0.95, ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "rfmodel = RandomForestClassifier(random_state = 100, criterion='entropy', max_depth=None, n_estimators=125)\n",
    "rfmodel = rfmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "rfpredictions = rfmodel.predict(ntest_vector)\n",
    "accuracyrf = accuracy_score(test_news[\"Label\"], rfpredictions)\n",
    "print(\"Random forest with tfid and bigram\", accuracyrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost/Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment = train_news\n",
    "test_sentiment = test_news\n",
    "\n",
    "train_sentiment = train_sentiment.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "for column in train_sentiment:\n",
    "    train_sentiment[column] = train_sentiment[column].apply(analize_sentiment)\n",
    "train_sentiment = train_sentiment + 10\n",
    "\n",
    "test_sentiment = test_sentiment.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "for column in test_sentiment:\n",
    "    test_sentiment[column] = test_sentiment[column].apply(analize_sentiment)\n",
    "test_sentiment = test_sentiment + 10\n",
    "\n",
    "XGB_model = XGBClassifier(random_state=100, criterion='entropy')\n",
    "gradiant = XGB_model.fit(train_sentiment, train_news['Label'])\n",
    "y_pred = gradiant.predict(test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6  91]\n",
      " [  4 120]]\n",
      "Sentiment Accuracy 0.5701357466063348\n",
      "f1_score 0.4511960432960162\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_news['Label'], y_pred))\n",
    "print(\"Sentiment Accuracy\", accuracy_score(test_news['Label'], y_pred))\n",
    "print(\"f1_score\", f1_score(test_news['Label'], y_pred, average='weighted'))\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment_weight = train_news\n",
    "test_sentiment_weight = test_news\n",
    "\n",
    "train_sentiment_weight = train_sentiment_weight.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "for column in train_sentiment_weight:\n",
    "    train_sentiment_weight[column] = train_sentiment_weight[column].apply(analize_sentiment)\n",
    "train_sentiment_weight = train_sentiment_weight + 10\n",
    "\n",
    "test_sentiment_weight = test_sentiment_weight.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "for column in test_sentiment_weight:\n",
    "    test_sentiment_weight[column] = test_sentiment_weight[column].apply(analize_sentiment)\n",
    "test_sentiment_weight = test_sentiment_weight + 10\n",
    "\n",
    "for column in train_news:\n",
    "    if not train_news[column].empty:\n",
    "        empty_data = train_news[column]\n",
    "    else:\n",
    "        weighted_data = train_news[column]\n",
    "\n",
    "weighted_XGB = XGBClassifier()\n",
    "weighted_XGB.fit(train_sentiment_weight, train_news['Label'], sample_weight=weighted_data)\n",
    "y_pred_weight = weighted_XGB.predict(test_sentiment_weight, ntree_limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Accuracy 0.43891402714932126\n",
      "F1 weighted 0.26776516121688154\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted Accuracy\", accuracy_score(test_news['Label'], y_pred_weight))\n",
    "print(\"F1 weighted\", f1_score(test_news['Label'], y_pred_weight, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All scores are printed out for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model 0.5610859728506787\n",
      "Random Forest 0.5656108597285068\n",
      "Sentiment Accuracy 0.5701357466063348\n",
      "Weighted Accuracy 0.43891402714932126\n"
     ]
    }
   ],
   "source": [
    "print(\"Base model\", accuracy1)\n",
    "print(\"Random Forest\", accuracyrf)\n",
    "print(\"Sentiment Accuracy\", accuracy_score(test_news['Label'], y_pred))\n",
    "print(\"Weighted Accuracy\", accuracy_score(test_news['Label'], y_pred_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
