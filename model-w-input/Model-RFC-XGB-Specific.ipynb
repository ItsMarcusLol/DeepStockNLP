{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize_sentiment(tweet):\n",
    "    analysis = TextBlob((str(tweet)))\n",
    "    return analysis.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the path for the CSV and put it in here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('J-F-N-input.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the train news and test news datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_news(day, month, year):\n",
    "    index = 0\n",
    "    dataset = []\n",
    "    d1 = datetime(year, month, day).date() \n",
    "    for date in news['Date']:\n",
    "        d = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        if d1 > d:\n",
    "            dataset.append(news.iloc[index])\n",
    "        index = index +1\n",
    "        df = pd.DataFrame(dataset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_news(day, month, year):\n",
    "    index = 0\n",
    "    dataset = []\n",
    "    d1 = datetime(year, month, day).date() \n",
    "    for date in news['Date']:\n",
    "        d = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        if d1 < d:\n",
    "            dataset.append(news.iloc[index])\n",
    "        index = index +1\n",
    "        df = pd.DataFrame(dataset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_column = (len(news.columns))\n",
    "# day, month, year\n",
    "train_news = get_train_news(8, 2, 2019)\n",
    "test_news = get_test_news(7, 2, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news_list = []\n",
    "for row in range (0, len(train_news.index)):\n",
    "    train_news_list.append(' '.join(str(k) for k in train_news.iloc[row,12:num_column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = CountVectorizer(min_df=0.01, max_df=0.8)\n",
    "news_vector = vectorize.fit_transform(train_news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TABLE OF FREQUENCY WORD DISTRIBUTION (26, 43)\n"
     ]
    }
   ],
   "source": [
    "print(\"THE TABLE OF FREQUENCY WORD DISTRIBUTION\", news_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model: Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "model = lr.fit(news_vector, train_news[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = vectorize.transform(test_news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  0\n",
       "Actual      \n",
       "0          6\n",
       "1          7"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(test_vector)\n",
    "pd.crosstab(test_news[\"Label\"], predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the baseline model accuracy 0.46153846153846156\n"
     ]
    }
   ],
   "source": [
    "accuracy1=accuracy_score(test_news[\"Label\"], predictions)\n",
    "print(\"the baseline model accuracy\", accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten words according to the baseline model         Word  Coefficient\n",
      "14      have     0.222310\n",
      "17      made     0.222310\n",
      "20     money     0.222310\n",
      "21      much     0.222310\n",
      "37     today     0.222310\n",
      "42       you     0.222310\n",
      "6     demise     0.208729\n",
      "18      maps     0.208729\n",
      "19  marriage     0.208729\n",
      "23        my     0.208729\n",
      "Last ten words according to the baseline model          Word  Coefficient\n",
      "35       tech    -0.171197\n",
      "40       week    -0.171197\n",
      "0     accused    -0.189059\n",
      "2       board    -0.189059\n",
      "9   executive    -0.189059\n",
      "10       exit    -0.189059\n",
      "27       over    -0.189059\n",
      "28    package    -0.189059\n",
      "33       sued    -0.189059\n",
      "1         and    -0.301376\n"
     ]
    }
   ],
   "source": [
    "words = vectorize.get_feature_names()\n",
    "coefficients = model.coef_.tolist()[0]\n",
    "coeffdf = pd.DataFrame({'Word' : words, 'Coefficient' : coefficients})\n",
    "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0,1])\n",
    "print(\"Top ten words according to the baseline model\", coeffdf.head(10))\n",
    "print(\"Last ten words according to the baseline model\", coeffdf.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest with tfid and bigram 0.46153846153846156\n"
     ]
    }
   ],
   "source": [
    "nvectorize = TfidfVectorizer(min_df=0.01, max_df=0.95, ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "rfmodel = RandomForestClassifier(random_state = 100, criterion='entropy', max_depth=None, n_estimators=125)\n",
    "rfmodel = rfmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "rfpredictions = rfmodel.predict(ntest_vector)\n",
    "accuracyrf = accuracy_score(test_news[\"Label\"], rfpredictions)\n",
    "print(\"Random forest with tfid and bigram\", accuracyrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost/Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { criterion } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:41:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcard\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_sentiment = copy.deepcopy(train_news)\n",
    "test_sentiment = copy.deepcopy(test_news)\n",
    "train_news2 = copy.deepcopy(train_news)\n",
    "test_news2 = copy.deepcopy(test_news)\n",
    "\n",
    "train_sentiment = train_sentiment.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "for column in train_sentiment:\n",
    "    train_sentiment[column] = train_sentiment[column].apply(analize_sentiment)\n",
    "    train_news2[column] = train_sentiment[column] + 10\n",
    "train_sentiment = train_sentiment + 10\n",
    "\n",
    "test_sentiment = test_sentiment.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "for column in test_sentiment:\n",
    "    test_sentiment[column] = test_sentiment[column].apply(analize_sentiment)\n",
    "    test_news2[column] = test_sentiment[column] + 10  \n",
    "test_sentiment = test_sentiment + 10\n",
    "\n",
    "XGB_model = XGBClassifier(random_state=100, criterion='entropy')\n",
    "gradiant = XGB_model.fit(train_sentiment, train_news['Label'])\n",
    "y_pred = gradiant.predict(test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 0]\n",
      " [7 0]]\n",
      "Sentiment Accuracy 0.46153846153846156\n",
      "f1_score 0.291497975708502\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_news['Label'], y_pred))\n",
    "print(\"Sentiment Accuracy\", accuracy_score(test_news['Label'], y_pred))\n",
    "print(\"f1_score\", f1_score(test_news['Label'], y_pred, average='weighted'))\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 - includes trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Sentiment Accuracy with Trend 0.6153846153846154\n",
      "f1_score 0.6153846153846153\n"
     ]
    }
   ],
   "source": [
    "train_news2 = train_news2.drop(['Date','Label'], axis = 1)\n",
    "test_news2 = test_news2.drop(['Date', 'Label'], axis = 1)\n",
    "\n",
    "XGB_model2 = XGBClassifier()\n",
    "gradiant2 = XGB_model2.fit(train_news2, train_news['Label'])\n",
    "y_pred2 = gradiant2.predict(test_news2)\n",
    "\n",
    "\n",
    "# print(confusion_matrix(test_news['Label'], y_pred))\n",
    "print(\"Sentiment Accuracy with Trend\", accuracy_score(test_news['Label'], y_pred2))\n",
    "print(\"f1_score\", f1_score(test_news['Label'], y_pred2, average='weighted'))\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted XGBoost (attempted to put weight towards headlines) Vanessa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "train_sentiment_weight = train_news\n",
    "test_sentiment_weight = test_news\n",
    "\n",
    "train_sentiment_weight = train_sentiment_weight.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "for column in train_sentiment_weight:\n",
    "    train_sentiment_weight[column] = train_sentiment_weight[column].apply(analize_sentiment)\n",
    "train_sentiment_weight = train_sentiment_weight + 10\n",
    "\n",
    "test_sentiment_weight = test_sentiment_weight.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "for column in test_sentiment_weight:\n",
    "    test_sentiment_weight[column] = test_sentiment_weight[column].apply(analize_sentiment)\n",
    "test_sentiment_weight = test_sentiment_weight + 10\n",
    "\n",
    "for column in train_news:\n",
    "    if not train_news[column].empty:\n",
    "        empty_data = train_news[column]\n",
    "    else:\n",
    "        weighted_data = train_news[column]\n",
    "\n",
    "weighted_XGB = XGBClassifier()\n",
    "weighted_XGB.fit(train_sentiment_weight, train_news['Label'], sample_weight=weighted_data)\n",
    "y_pred_weight = weighted_XGB.predict(test_sentiment_weight, ntree_limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Accuracy 0.46153846153846156\n",
      "F1 weighted 0.291497975708502\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted Accuracy\", accuracy_score(test_news['Label'], y_pred_weight))\n",
    "print(\"F1 weighted\", f1_score(test_news['Label'], y_pred_weight, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted - using trends (Erika)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_weight(train_sentiment):\n",
    "    train_weight = []\n",
    "\n",
    "    for across in range(len(train_sentiment)):\n",
    "        for i in range(10):\n",
    "            train_weight.append(0.5)\n",
    "        for x in train_sentiment:\n",
    "            if x not in ['Date', 'Label', '1', '2','3','4','5','6','7','8','9', '10'] :\n",
    "                if train_sentiment[x][across] == 10.0:\n",
    "                    train_weight.append(0.5)\n",
    "                else:\n",
    "                    train_weight.append(0.8)\n",
    "    return train_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_weight(train_sentiment, test_sentiment):\n",
    "\n",
    "    test_weight = []\n",
    "    j = len(train_sentiment)\n",
    "    for across in range(len(test_sentiment)):\n",
    "        for i in range(10):\n",
    "            test_weight.append(0.5)\n",
    "        for x in test_sentiment:\n",
    "            if x not in ['Date', 'Label', '1', '2','3','4','5','6','7','8','9', '10'] :\n",
    "                if test_sentiment[x][j] == 10.0:\n",
    "                    test_weight.append(0.5)\n",
    "                else:\n",
    "                    test_weight.append(0.8)\n",
    "        j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:41:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Sentiment Accuracy with Trend with weight 0.6153846153846154\n",
      "f1_score 0.6153846153846153\n"
     ]
    }
   ],
   "source": [
    "train_weight = get_train_weight(train_sentiment)\n",
    "test_weight = get_test_weight(train_sentiment, test_sentiment)\n",
    "\n",
    "XGB_model4 = XGBClassifier()\n",
    "\n",
    "num_round = 2\n",
    "param = {'max_depth' : 2, 'eta': 1, 'objective':'binary:logistic' }\n",
    "gradiant4 = XGB_model4.fit(train_news2, train_news['Label'], feature_weights = train_weight)\n",
    "y_pred4 = gradiant4.predict(test_news2)\n",
    "\n",
    "print(\"Sentiment Accuracy with Trend with weight\", accuracy_score(test_news['Label'], y_pred4))\n",
    "print(\"f1_score\", f1_score(test_news['Label'], y_pred4, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Weighted Accuracy 2 0.46153846153846156\n",
      "F1 weighted 0.291497975708502\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentiment Weighted Accuracy 2\", accuracy_score(test_news['Label'], y_pred_weight))\n",
    "print(\"F1 weighted\", f1_score(test_news['Label'], y_pred_weight, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "## doesn't work- needs to be based on row numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtrain = xgb.DMatrix( data = train_news2, label = train_news['Label'], weight = train_weight)\n",
    "# dtest = xgb.DMatrix(data = test_news2, label = test_news['Label'], weight = test_weight)\n",
    "# params  = dict(max_depth=2, eta=1, verbose=0, nthread=2, eval_metric = \"auc\",\n",
    "#               objective=\"binary:logistic\")\n",
    "\n",
    "# m = xgb.train(params, dtrain)\n",
    "\n",
    "# y_predictions = m.predict(dtest)\n",
    "\n",
    "# i = 0\n",
    "# y = []\n",
    "# for p in y_predictions:\n",
    "#     if p > 0.49:\n",
    "#         y.append(1)\n",
    "#     else:\n",
    "#         y.append(0)\n",
    "#     i = i + 1\n",
    "# print(\"Sentiment Accuracy with Trend with weight\", accuracy_score(test_news['Label'], y))\n",
    "# print(\"f1_score\", f1_score(test_news['Label'], y, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All scores are printed out for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model 0.46153846153846156\n",
      "Random Forest 0.46153846153846156\n",
      "Sentiment Accuracy 0.46153846153846156\n",
      "Sentiment Accuracy with Trends 0.6153846153846154\n",
      "Weighted Accuracy (Vans) 0.46153846153846156\n",
      "Sentiment Accuracy with Trend with weight (Erika) 0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "print(\"Base model\", accuracy1)\n",
    "print(\"Random Forest\", accuracyrf)\n",
    "print(\"Sentiment Accuracy\", accuracy_score(test_news['Label'], y_pred))\n",
    "print(\"Sentiment Accuracy with Trends\", accuracy_score(test_news['Label'], y_pred2))\n",
    "print(\"Weighted Accuracy (Vans)\", accuracy_score(test_news['Label'], y_pred_weight))\n",
    "print(\"Sentiment Accuracy with Trend with weight (Erika)\", accuracy_score(test_news['Label'], y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
