{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize_sentiment(tweet):\n",
    "    analysis = TextBlob((str(tweet)))\n",
    "    return analysis.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the path for the CSV and put it in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"/Users/vanessahoang/documents/deepstocknlp/data/18-20-csv/2018-2020_model_input.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the test news and training news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_news(day, month, year):\n",
    "    index = 0\n",
    "    dataset = []\n",
    "    d1 = datetime(year, month, day).date() \n",
    "    for date in news['Date']:\n",
    "        d = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        if d1 > d:\n",
    "            dataset.append(news.iloc[index])\n",
    "        index = index +1\n",
    "        df = pd.DataFrame(dataset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_news(day, month, year):\n",
    "    index = 0\n",
    "    dataset = []\n",
    "    d1 = datetime(year, month, day).date() \n",
    "    for date in news['Date']:\n",
    "        d = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        if d1 < d:\n",
    "            dataset.append(news.iloc[index])\n",
    "        index = index +1\n",
    "        df = pd.DataFrame(dataset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_column = (len(news.columns))\n",
    "# day, month, year\n",
    "train_news = get_train_news(15, 1, 2018)\n",
    "test_news = get_test_news(14, 1, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range (0, len(train_news.index)):\n",
    "    train_news_list.append(' '.join(str(k) for k in train_news.iloc[row,12:num_column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = CountVectorizer(min_df=0.01, max_df=0.8)\n",
    "news_vector = vectorize.fit_transform(train_news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TABLE OF FREQUENCY WORD DISTRIBUTION (8, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"THE TABLE OF FREQUENCY WORD DISTRIBUTION\", news_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "model = lr.fit(news_vector, train_news[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = vectorize.transform(test_news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    1\n",
       "Actual        \n",
       "0          344\n",
       "1          380"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(test_vector)\n",
    "pd.crosstab(test_news[\"Label\"], predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the baseline model accuracy 0.5248618784530387\n"
     ]
    }
   ],
   "source": [
    "accuracy1=accuracy_score(test_news[\"Label\"], predictions)\n",
    "print(\"the baseline model accuracy\", accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorize.get_feature_names()\n",
    "coefficients = model.coef_.tolist()[0]\n",
    "coeffdf = pd.DataFrame({'Word' : words, 'Coefficient' : coefficients})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten words according to the baseline model         Word  Coefficient\n",
      "9     google     0.506022\n",
      "17      with     0.363446\n",
      "4   changing     0.190605\n",
      "6      codes     0.190605\n",
      "8    country     0.190605\n",
      "1     albums     0.172840\n",
      "12    online     0.172840\n",
      "13    photos     0.172840\n",
      "14   sharing     0.172840\n",
      "0    against     0.142577\n",
      "Last ten words according to the baseline model             Word  Coefficient\n",
      "14       sharing     0.172840\n",
      "0        against     0.142577\n",
      "2         author     0.142577\n",
      "3           bias     0.142577\n",
      "5       claiming     0.142577\n",
      "7   conservative     0.142577\n",
      "10          memo     0.142577\n",
      "11           men     0.142577\n",
      "15          sues     0.142577\n",
      "16         white     0.142577\n"
     ]
    }
   ],
   "source": [
    "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0,1])\n",
    "print(\"Top ten words according to the baseline model\", coeffdf.head(10))\n",
    "print(\"Last ten words according to the baseline model\", coeffdf.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Bigram and TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFID TRANSFORMATION DATAFRAME SHAPE (8, 20)\n",
      "Logistics Regression with Bigram and TFID 0.5248618784530387\n",
      "                Word  Coefficient\n",
      "19       with google     0.237026\n",
      "4   changing country     0.148222\n",
      "6         codes with     0.148222\n",
      "8      country codes     0.148222\n",
      "10        google nan     0.148222\n",
      "1        albums with     0.134599\n",
      "11     google photos     0.134599\n",
      "14     online albums     0.134599\n",
      "15        photos nan     0.134599\n",
      "16    sharing online     0.134599\n",
      "                  Word  Coefficient\n",
      "0        against white     0.104129\n",
      "2          author sues     0.104129\n",
      "3         bias against     0.104129\n",
      "5        claiming bias     0.104129\n",
      "7     conservative men     0.104129\n",
      "9          google memo     0.104129\n",
      "12         memo author     0.104129\n",
      "13             men nan     0.104129\n",
      "17       sues claiming     0.104129\n",
      "18  white conservative     0.104129\n"
     ]
    }
   ],
   "source": [
    "nvectorize = TfidfVectorizer(min_df=0.01, max_df=0.90, ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "print(\"TFID TRANSFORMATION DATAFRAME SHAPE\", news_nvector.shape)\n",
    "nmodel = lr.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range (0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "npredictions = nmodel.predict(ntest_vector)\n",
    "pd.crosstab(test_news[\"Label\"], npredictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "accuracy2 = accuracy_score(test_news['Label'], npredictions)\n",
    "print(\"Logistics Regression with Bigram and TFID\", accuracy2)\n",
    "nwords = nvectorize.get_feature_names()\n",
    "ncoefficients = nmodel.coef_.tolist()[0]\n",
    "ncoeffdf = pd.DataFrame({'Word':nwords, 'Coefficient':ncoefficients})\n",
    "ncoeffdf = ncoeffdf.sort_values(['Coefficient', 'Word'], ascending=[0,1])\n",
    "ncoeffdf.head(10)\n",
    "ncoeffdf.tail(10)\n",
    "print(ncoeffdf.head(10))\n",
    "print(ncoeffdf.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest with tfid and bigram 0.4723756906077348\n"
     ]
    }
   ],
   "source": [
    "nvectorize = TfidfVectorizer(min_df=0.01, max_df=0.95, ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "rfmodel = RandomForestClassifier(random_state = 100, criterion='entropy')\n",
    "rfmodel = rfmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "rfpredictions = rfmodel.predict(ntest_vector)\n",
    "accuracyrf = accuracy_score(test_news[\"Label\"], rfpredictions)\n",
    "print(\"Random forest with tfid and bigram\", accuracyrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy:  0.5248618784530387\n"
     ]
    }
   ],
   "source": [
    "nvectorize = TfidfVectorizer(min_df=0.01, max_df=0.9,ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "nbmodel = MultinomialNB(alpha=0.5)\n",
    "nbmodel = nbmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "nbpredictions = nbmodel.predict(ntest_vector)\n",
    "nbaccuracy = accuracy_score(test_news['Label'], nbpredictions)\n",
    "print(\"Naive Bayes accuracy: \", nbaccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbmodel = GradientBoostingClassifier(random_state = 52)\n",
    "gbmodel = gbmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "gbpredictions = gbmodel.predict(ntest_vector.toarray())\n",
    "gbaccuracy = accuracy_score(test_news['Label'], gbpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX OF THE GRADIANT BOOSTING\n",
      "[[343   1]\n",
      " [378   2]]\n",
      "Gradient boosting accuracy:  0.47651933701657456\n"
     ]
    }
   ],
   "source": [
    "print(\"CONFUSION MATRIX OF THE GRADIANT BOOSTING\")\n",
    "print(confusion_matrix(test_news['Label'], gbpredictions))\n",
    "\n",
    "print(\"Gradient boosting accuracy: \", gbaccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 21)\n",
      "TRIGRAM ACCURACY 0.5248618784530387\n",
      "trigram top ten word distribution                       Word  Coefficient\n",
      "4   changing country codes     0.146528\n",
      "6        codes with google     0.146528\n",
      "8       country codes with     0.146528\n",
      "10          google nan nan     0.146528\n",
      "19         with google nan     0.146528\n",
      "1       albums with google     0.133761\n",
      "11       google photos nan     0.133761\n",
      "14      online albums with     0.133761\n",
      "15          photos nan nan     0.133761\n",
      "16   sharing online albums     0.133761\n",
      "trigarm last ten word distribution                           Word  Coefficient\n",
      "0   against white conservative     0.103611\n",
      "2         author sues claiming     0.103611\n",
      "3           bias against white     0.103611\n",
      "5        claiming bias against     0.103611\n",
      "7         conservative men nan     0.103611\n",
      "9           google memo author     0.103611\n",
      "12            memo author sues     0.103611\n",
      "13                 men nan nan     0.103611\n",
      "17          sues claiming bias     0.103611\n",
      "18      white conservative men     0.103611\n"
     ]
    }
   ],
   "source": [
    "n3vectorize = TfidfVectorizer(min_df=0.0001, max_df=0.9, ngram_range=(3,3))\n",
    "news_n3vector = n3vectorize.fit_transform(train_news_list)\n",
    "print(news_n3vector.shape)\n",
    "\n",
    "n3model = lr.fit(news_n3vector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    \n",
    "#     print(\"P\" ,' '.join(str(x) for x in test_news.iloc[row,12:num_column]) )\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))\n",
    "n3test_vector = n3vectorize.transform(test_news_list)\n",
    "\n",
    "n3predictions = n3model.predict(n3test_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], n3predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy3 = accuracy_score(test_news['Label'], n3predictions)\n",
    "print(\"TRIGRAM ACCURACY\", accuracy3)\n",
    "\n",
    "n3words = n3vectorize.get_feature_names()\n",
    "n3coefficients = n3model.coef_.tolist()[0]\n",
    "n3coeffdf = pd.DataFrame({'Word':n3words, 'Coefficient':n3coefficients})\n",
    "n3coeffdf = n3coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0,1])\n",
    "print(\"trigram top ten word distribution\", n3coeffdf.head(10))\n",
    "print(\"trigarm last ten word distribution\", n3coeffdf.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment = train_news\n",
    "test_sentiment = test_news\n",
    "# train_sentiment = train_sentiment.drop(['Date', 'Label'], axis=1)\n",
    "train_sentiment = train_sentiment.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "for column in train_sentiment:\n",
    "    train_sentiment[column] = train_sentiment[column].apply(analize_sentiment)\n",
    "train_sentiment = train_sentiment + 10\n",
    "\n",
    "# test_sentiment = test_sentiment.drop(['Date', 'Label'], axis=1)\n",
    "test_sentiment = test_sentiment.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "for column in test_sentiment:\n",
    "    test_sentiment[column] = test_sentiment[column].apply(analize_sentiment)\n",
    "test_sentiment = test_sentiment + 10\n",
    "\n",
    "XGB_model = XGBClassifier()\n",
    "gradiant = XGB_model.fit(train_sentiment, train_news['Label'])\n",
    "y_pred = gradiant.predict(test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_news['Label'], y_pred))\n",
    "print(\"Sentiment Accuracy\", accuracy_score(test_news['Label'], y_pred))\n",
    "print(\"f1_score\", f1_score(test_news['Label'], y_pred, average='weighted'))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All scores are printed out for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Base model\", accuracy1)\n",
    "print(\"Logistic Regression\",accuracy2)\n",
    "print(\"Random Forest\", accuracyrf)\n",
    "print(\"Naive Bayes\", nbaccuracy)\n",
    "print(\"Gradient Boost\", gbaccuracy)\n",
    "print(\"Trigram\", accuracy3)\n",
    "print(\"Sentiment Accuracy\", accuracy_score(test_news['Label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempted to plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x = test_news['Date']\n",
    "\n",
    "# plt.plot(x, test_news['Label'], label='Actual')\n",
    "# plt.plot(x, y_pred, 'bo', label='XGB')\n",
    "# #plt.plot(x, n3predictions, 'bo', label='Trigram')\n",
    "# #plt.plot(x, gbpredictions, 'bo', label='Gradient Boost')\n",
    "# #plt.plot(x, nbpredictions, 'bo', label='Naive Bayes')\n",
    "# #plt.plot(x, rfpredictions, 'bo', label='Random Forest')\n",
    "# #plt.plot(x, npredictions, 'bo', label='Logistic Regression')\n",
    "# #plt.plot(x, predictions, 'bo', label='Base')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
