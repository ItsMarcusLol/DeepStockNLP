{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize_sentiment(tweet):\n",
    "    analysis = TextBlob((str(tweet)))\n",
    "    return analysis.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the path for the CSV and put it in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('2018-2020_model_input.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the test news and training news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_news(day, month, year):\n",
    "    index = 0\n",
    "    dataset = []\n",
    "    d1 = datetime(year, month, day).date() \n",
    "    for date in news['Date']:\n",
    "        d = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        if d1 > d:\n",
    "            dataset.append(news.iloc[index])\n",
    "        index = index +1\n",
    "        df = pd.DataFrame(dataset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_news(day, month, year):\n",
    "    index = 0\n",
    "    dataset = []\n",
    "    d1 = datetime(year, month, day).date() \n",
    "    for date in news['Date']:\n",
    "        d = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        if d1 < d:\n",
    "            dataset.append(news.iloc[index])\n",
    "        index = index +1\n",
    "        df = pd.DataFrame(dataset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_column = (len(news.columns))\n",
    "# day, month, year\n",
    "train_news = get_train_news(15, 1, 2020)\n",
    "test_news = get_test_news(14, 1, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range (0, len(train_news.index)):\n",
    "    train_news_list.append(' '.join(str(k) for k in train_news.iloc[row,12:num_column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = CountVectorizer(min_df=0.01, max_df=0.8)\n",
    "news_vector = vectorize.fit_transform(train_news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TABLE OF FREQUENCY WORD DISTRIBUTION (511, 46)\n"
     ]
    }
   ],
   "source": [
    "print(\"THE TABLE OF FREQUENCY WORD DISTRIBUTION\", news_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "model = lr.fit(news_vector, train_news[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = vectorize.transform(test_news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0    1\n",
       "Actual            \n",
       "0          14   83\n",
       "1          14  110"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(test_vector)\n",
    "pd.crosstab(test_news[\"Label\"], predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the baseline model accuracy 0.5610859728506787\n"
     ]
    }
   ],
   "source": [
    "accuracy1=accuracy_score(test_news[\"Label\"], predictions)\n",
    "print(\"the baseline model accuracy\", accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorize.get_feature_names()\n",
    "coefficients = model.coef_.tolist()[0]\n",
    "coeffdf = pd.DataFrame({'Word' : words, 'Coefficient' : coefficients})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten words according to the baseline model         Word  Coefficient\n",
      "39       why     1.023201\n",
      "10  children     0.666932\n",
      "2         an     0.641347\n",
      "42   workers     0.532837\n",
      "17      from     0.514270\n",
      "5         as     0.504116\n",
      "1     amazon     0.460666\n",
      "11     china     0.448186\n",
      "33      that     0.422779\n",
      "22        it     0.397774\n",
      "Last ten words according to the baseline model         Word  Coefficient\n",
      "24       new    -0.451398\n",
      "6         be    -0.454282\n",
      "0      after    -0.514281\n",
      "44       you    -0.690286\n",
      "3        and    -0.764320\n",
      "15  facebook    -0.822030\n",
      "13  dealbook    -0.825163\n",
      "38        up    -0.832346\n",
      "23       its    -0.886823\n",
      "43      york    -1.023077\n"
     ]
    }
   ],
   "source": [
    "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0,1])\n",
    "print(\"Top ten words according to the baseline model\", coeffdf.head(10))\n",
    "print(\"Last ten words according to the baseline model\", coeffdf.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Bigram and TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFID TRANSFORMATION DATAFRAME SHAPE (511, 18)\n",
      "Logistics Regression with Bigram and TFID 0.5565610859728507\n",
      "               Word  Coefficient\n",
      "13          in tech     1.045437\n",
      "4      briefing nan     0.301139\n",
      "10       google nan     0.266718\n",
      "7        for google     0.263663\n",
      "9         google is     0.088573\n",
      "17    your thursday     0.043000\n",
      "16       the google     0.036561\n",
      "3   briefing google     0.030690\n",
      "12      google your    -0.077724\n",
      "6      facebook and    -0.202178\n",
      "                 Word  Coefficient\n",
      "12        google your    -0.077724\n",
      "6        facebook and    -0.202178\n",
      "11          google to    -0.231870\n",
      "2   briefing dealbook    -0.233130\n",
      "5   dealbook briefing    -0.565905\n",
      "8          google and    -0.719672\n",
      "14             in the    -0.784662\n",
      "1          and google    -0.845605\n",
      "0        and facebook    -1.053128\n",
      "15           new york    -1.229554\n"
     ]
    }
   ],
   "source": [
    "nvectorize = TfidfVectorizer(min_df=0.01, max_df=0.90, ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "print(\"TFID TRANSFORMATION DATAFRAME SHAPE\", news_nvector.shape)\n",
    "nmodel = lr.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range (0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "npredictions = nmodel.predict(ntest_vector)\n",
    "pd.crosstab(test_news[\"Label\"], npredictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "accuracy2 = accuracy_score(test_news['Label'], npredictions)\n",
    "print(\"Logistics Regression with Bigram and TFID\", accuracy2)\n",
    "nwords = nvectorize.get_feature_names()\n",
    "ncoefficients = nmodel.coef_.tolist()[0]\n",
    "ncoeffdf = pd.DataFrame({'Word':nwords, 'Coefficient':ncoefficients})\n",
    "ncoeffdf = ncoeffdf.sort_values(['Coefficient', 'Word'], ascending=[0,1])\n",
    "ncoeffdf.head(10)\n",
    "ncoeffdf.tail(10)\n",
    "print(ncoeffdf.head(10))\n",
    "print(ncoeffdf.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest with tfid and bigram 0.5656108597285068\n"
     ]
    }
   ],
   "source": [
    "nvectorize = TfidfVectorizer(min_df=0.01, max_df=0.95, ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "rfmodel = RandomForestClassifier(random_state = 100, criterion='entropy')\n",
    "rfmodel = rfmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "rfpredictions = rfmodel.predict(ntest_vector)\n",
    "accuracyrf = accuracy_score(test_news[\"Label\"], rfpredictions)\n",
    "print(\"Random forest with tfid and bigram\", accuracyrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy:  0.5565610859728507\n"
     ]
    }
   ],
   "source": [
    "nvectorize = TfidfVectorizer(min_df=0.01, max_df=0.9,ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "nbmodel = MultinomialNB(alpha=0.5)\n",
    "nbmodel = nbmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "nbpredictions = nbmodel.predict(ntest_vector)\n",
    "nbaccuracy = accuracy_score(test_news['Label'], nbpredictions)\n",
    "print(\"Naive Bayes accuracy: \", nbaccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbmodel = GradientBoostingClassifier(random_state = 52)\n",
    "gbmodel = gbmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "gbpredictions = gbmodel.predict(ntest_vector.toarray())\n",
    "gbaccuracy = accuracy_score(test_news['Label'], gbpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX OF THE GRADIANT BOOSTING\n",
      "[[  3  94]\n",
      " [  6 118]]\n",
      "Gradient boosting accuracy:  0.5475113122171946\n"
     ]
    }
   ],
   "source": [
    "print(\"CONFUSION MATRIX OF THE GRADIANT BOOSTING\")\n",
    "print(confusion_matrix(test_news['Label'], gbpredictions))\n",
    "\n",
    "print(\"Gradient boosting accuracy: \", gbaccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(511, 2074)\n",
      "TRIGRAM ACCURACY 0.5294117647058824\n",
      "trigram top ten word distribution                          Word  Coefficient\n",
      "53         against google nan     0.313074\n",
      "2066  your wednesday briefing     0.273938\n",
      "2060     your monday briefing     0.241318\n",
      "1964             week in tech     0.237216\n",
      "515         employees nan nan     0.236086\n",
      "254      brexit your thursday     0.229267\n",
      "730        google brexit your     0.229267\n",
      "1880     turkey google brexit     0.229267\n",
      "816            google nan nan     0.218454\n",
      "431              data nan nan     0.212612\n",
      "trigarm last ten word distribution                       Word  Coefficient\n",
      "138     apple hires google    -0.205112\n",
      "341          chief nan nan    -0.205112\n",
      "741       google chief nan    -0.205112\n",
      "952     hires google chief    -0.205112\n",
      "106   and google translate    -0.209435\n",
      "36             ads nan nan    -0.214331\n",
      "23       accuses google of    -0.216035\n",
      "849     google returns nan    -0.290073\n",
      "1512       returns nan nan    -0.290073\n",
      "712    google and facebook    -0.517587\n"
     ]
    }
   ],
   "source": [
    "n3vectorize = TfidfVectorizer(min_df=0.0001, max_df=0.9, ngram_range=(3,3))\n",
    "news_n3vector = n3vectorize.fit_transform(train_news_list)\n",
    "print(news_n3vector.shape)\n",
    "\n",
    "n3model = lr.fit(news_n3vector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0, len(test_news.index)):\n",
    "    \n",
    "#     print(\"P\" ,' '.join(str(x) for x in test_news.iloc[row,12:num_column]) )\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:num_column]))\n",
    "n3test_vector = n3vectorize.transform(test_news_list)\n",
    "\n",
    "n3predictions = n3model.predict(n3test_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], n3predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy3 = accuracy_score(test_news['Label'], n3predictions)\n",
    "print(\"TRIGRAM ACCURACY\", accuracy3)\n",
    "\n",
    "n3words = n3vectorize.get_feature_names()\n",
    "n3coefficients = n3model.coef_.tolist()[0]\n",
    "n3coeffdf = pd.DataFrame({'Word':n3words, 'Coefficient':n3coefficients})\n",
    "n3coeffdf = n3coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0,1])\n",
    "print(\"trigram top ten word distribution\", n3coeffdf.head(10))\n",
    "print(\"trigarm last ten word distribution\", n3coeffdf.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment = train_news\n",
    "test_sentiment = test_news\n",
    "# train_sentiment = train_sentiment.drop(['Date', 'Label'], axis=1)\n",
    "train_sentiment = train_sentiment.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "for column in train_sentiment:\n",
    "    train_sentiment[column] = train_sentiment[column].apply(analize_sentiment)\n",
    "train_sentiment = train_sentiment + 10\n",
    "\n",
    "# test_sentiment = test_sentiment.drop(['Date', 'Label'], axis=1)\n",
    "test_sentiment = test_sentiment.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "for column in test_sentiment:\n",
    "    test_sentiment[column] = test_sentiment[column].apply(analize_sentiment)\n",
    "test_sentiment = test_sentiment + 10\n",
    "\n",
    "XGB_model = XGBClassifier()\n",
    "gradiant = XGB_model.fit(train_sentiment, train_news['Label'])\n",
    "y_pred = gradiant.predict(test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6  91]\n",
      " [  4 120]]\n",
      "Sentiment Accuracy 0.5701357466063348\n",
      "f1_score 0.4511960432960162\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_news['Label'], y_pred))\n",
    "print(\"Sentiment Accuracy\", accuracy_score(test_news['Label'], y_pred))\n",
    "print(\"f1_score\", f1_score(test_news['Label'], y_pred, average='weighted'))\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All scores are printed out for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model 0.5610859728506787\n",
      "Logistic Regression 0.5565610859728507\n",
      "Random Forest 0.5656108597285068\n",
      "Naive Bayes 0.5565610859728507\n",
      "Gradient Boost 0.5475113122171946\n",
      "Trigram 0.5294117647058824\n",
      "Sentiment Accuracy 0.5701357466063348\n"
     ]
    }
   ],
   "source": [
    "print(\"Base model\", accuracy1)\n",
    "print(\"Logistic Regression\",accuracy2)\n",
    "print(\"Random Forest\", accuracyrf)\n",
    "print(\"Naive Bayes\", nbaccuracy)\n",
    "print(\"Gradient Boost\", gbaccuracy)\n",
    "print(\"Trigram\", accuracy3)\n",
    "print(\"Sentiment Accuracy\", accuracy_score(test_news['Label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempted to plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x = test_news['Date']\n",
    "\n",
    "# plt.plot(x, test_news['Label'], label='Actual')\n",
    "# plt.plot(x, y_pred, 'bo', label='XGB')\n",
    "# #plt.plot(x, n3predictions, 'bo', label='Trigram')\n",
    "# #plt.plot(x, gbpredictions, 'bo', label='Gradient Boost')\n",
    "# #plt.plot(x, nbpredictions, 'bo', label='Naive Bayes')\n",
    "# #plt.plot(x, rfpredictions, 'bo', label='Random Forest')\n",
    "# #plt.plot(x, npredictions, 'bo', label='Logistic Regression')\n",
    "# #plt.plot(x, predictions, 'bo', label='Base')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
