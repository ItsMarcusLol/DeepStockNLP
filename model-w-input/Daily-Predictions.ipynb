{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "# import datetime\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob\n",
    "from xgboost import XGBClassifier\n",
    "# import xgboost as xgb\n",
    "import requests\n",
    "import json\n",
    "# import numpy as np\n",
    "# import json\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to Train and Test on last 270 days put true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_270 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analize_sentiment(tweet):\n",
    "    if str(tweet) == \"nan\":\n",
    "        analysis = TextBlob((str(tweet)))\n",
    "        return analysis.polarity\n",
    "    else:\n",
    "        analysis = TextBlob((str(tweet)))\n",
    "        if analysis.polarity == 0.0:\n",
    "            return (analysis.polarity + .05)\n",
    "    return analysis.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the path for the CSV and put it in here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_H = pd.read_csv(r'C:/Users/dcard/Cap-Repo/DeepStockNLP/Data/18-20-csv/2018-2021-input-2-22-21.csv')\n",
    "# saved_H = pd.read_csv(r'C:/Users/dcard/Cap-Repo/DeepStockNLP/Data/20-21-csv/2020-2021-input-2-22-21.csv')\n",
    "# saved_H = pd.read_csv(r'C:/Users/dcard/Cap-Repo/DeepStockNLP/Data/20-21-csv/2020-2021-input-3-19-21.csv')\n",
    "\n",
    "# saved_H = pd.read_csv(r'C:/Users/dcard/Cap-Repo/DeepStockNLP/Data/20-21-csv/2020-2021-input-4-10-21.csv')\n",
    "saved_H = pd.read_csv(r'C:/Users/dcard/Cap-Repo/DeepStockNLP/Data/20-21-csv/GOOGL-2021-input-4-26-21.csv')\n",
    "ticker = \"GOOGL\"\n",
    "# stocks = ['google', 'googl']\n",
    "stock = \"google\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "getPW = {'googl':60, 'tsla':20, 'aapl':0.75, 'amzn':50, 'ba':0, 'msft':7, 'dell':3, 'wmt':0, 'tgt': 0.948, 'F': 1.22}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = []\n",
    "for x in saved_H:\n",
    "    col.append(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_270 == True: \n",
    "    length = len(saved_H)\n",
    "    start = length - 270\n",
    "    new_news = []\n",
    "    i = 0\n",
    "  \n",
    "    for x in saved_H['Date']:\n",
    "        if i >= start:\n",
    "            row = []\n",
    "            for y in col:\n",
    "                row.append(saved_H[y][i])\n",
    "            new_news.append(row)\n",
    "            \n",
    "        i = i +1\n",
    "\n",
    "    \n",
    "    news = pd.DataFrame(new_news, columns=col)\n",
    "    \n",
    "else:\n",
    "    news = saved_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the train news and test news datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_news(day, month, year, data):\n",
    "    index = 0\n",
    "    dataset = []\n",
    "    d1 = datetime(year, month, day).date() \n",
    "    for date in data['Date']:\n",
    "        d = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        if d1 > d:\n",
    "            dataset.append(data.iloc[index])\n",
    "        index = index +1\n",
    "        df = pd.DataFrame(dataset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_news(day, month, year,data):\n",
    "    index = 0\n",
    "    dataset = []\n",
    "    d1 = datetime(year, month, day).date() \n",
    "    for date in data['Date']:\n",
    "        d = datetime.strptime(date, '%Y-%m-%d').date()\n",
    "        if d1 < d:\n",
    "            dataset.append(data.iloc[index])\n",
    "        index = index +1\n",
    "        df = pd.DataFrame(dataset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train and Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_column = (len(news.columns))\n",
    "# day, month, year\n",
    "\n",
    "train_date = news['Date'][(int(len(news) * .7))]\n",
    "year = train_date[0:4]\n",
    "month = train_date[5:7]\n",
    "day = train_date[8:10]\n",
    "\n",
    "test_date =  news['Date'][(int(len(news) * .7)-1)]\n",
    " \n",
    "tst_day =  test_date[8:10]\n",
    "tst_month = test_date[5:7]\n",
    "tst_year =test_date[0:4]\n",
    "\n",
    "\n",
    "train_news = get_train_news(int(day), int(month), int(year), news)\n",
    "test_news = get_test_news(int(tst_day), int(tst_month), int(tst_year), news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news_list = []\n",
    "for row in range (0, len(train_news.index)):\n",
    "    train_news_list.append(' '.join(str(k) for k in train_news.iloc[row,12:num_column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost/Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sentiment = copy.deepcopy(train_news)\n",
    "# test_sentiment = copy.deepcopy(test_news)\n",
    "# train_news2 = copy.deepcopy(train_news)\n",
    "# test_news2 = copy.deepcopy(test_news)\n",
    "\n",
    "# train_sentiment = train_sentiment.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "# for column in train_sentiment:\n",
    "# #     print(\"c\",column)\n",
    "#     train_sentiment[column] = train_sentiment[column].apply(analize_sentiment)\n",
    "#     train_news2[column] = train_sentiment[column] \n",
    "# train_sentiment = train_sentiment \n",
    "# print(train_sentiment)\n",
    "# test_sentiment = test_sentiment.drop(['Date', 'Label', '1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "# for column in test_sentiment:\n",
    "#     test_sentiment[column] = test_sentiment[column].apply(analize_sentiment)\n",
    "#     test_news2[column] = test_sentiment[column]  \n",
    "# test_sentiment = test_sentiment \n",
    "\n",
    "# XGB_model = XGBClassifier(random_state=100)\n",
    "# gradiant = XGB_model.fit(train_sentiment, train_news['Label'])\n",
    "# y_pred = gradiant.predict(test_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(confusion_matrix(test_news['Label'], y_pred))\n",
    "# print(\"Sentiment Accuracy\", accuracy_score(test_news['Label'], y_pred))\n",
    "# print(\"f1_score\", f1_score(test_news['Label'], y_pred, average='weighted'))\n",
    "# #print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model- scaling positive weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment_weight = train_news\n",
    "test_sentiment_weight = test_news\n",
    "weighted_data=[]\n",
    "empty_data=[]\n",
    "\n",
    "# train_sentiment_weight = train_sentiment_weight.drop(['Date', 'Label','1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "train_sentiment_weight = train_sentiment_weight.drop(['Date', 'Label'], axis=1)\n",
    "for column in train_sentiment_weight:\n",
    "    train_sentiment_weight[column] = train_sentiment_weight[column].apply(analize_sentiment)\n",
    "train_sentiment_weight = train_sentiment_weight \n",
    "\n",
    "# test_sentiment_weight = test_sentiment_weight.drop(['Date', 'Label','1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "test_sentiment_weight = test_sentiment_weight.drop(['Date', 'Label'], axis=1)\n",
    "for column in test_sentiment_weight:\n",
    "    test_sentiment_weight[column] = test_sentiment_weight[column].apply(analize_sentiment)\n",
    "test_sentiment_weight = test_sentiment_weight \n",
    "\n",
    "for column in train_news:\n",
    "    if not train_news[column].empty:\n",
    "        empty_data = train_news[column]\n",
    "    else:\n",
    "        weighted_data = train_news[column]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 108\n",
      "0: 81\n",
      "27\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "scale_weight = 0\n",
    "count = 0\n",
    "count0 = 0\n",
    "for x in train_news['Label']:\n",
    "    if x == 1:\n",
    "        count = count + 1\n",
    "    if x == 0:\n",
    "        count0 = count0 + 1\n",
    "print(\"1:\",count)\n",
    "print(\"0:\",count0)\n",
    "print(count - count0)\n",
    "n = (count - count0 )\n",
    "# scale_weight =((n + 9) // 10 * 10) \n",
    "\n",
    "# if scale_weight < 0:\n",
    "#     scale_weight = 0;\n",
    "scale_weight = count0/count\n",
    "\n",
    "print(scale_weight)\n",
    "\n",
    "scale_weight = getPW[ticker.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcard\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:18:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "weighted_XGB1 = XGBClassifier(scale_pos_weight = scale_weight)\n",
    "weighted_XGB1.fit(train_sentiment_weight, train_news['Label'], sample_weight = weighted_data)\n",
    "# print(train_sentiment_weight)\n",
    "y_pred_weight1 = weighted_XGB1.predict(test_sentiment_weight)\n",
    "# print(train_sentiment_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Accuracy 0.5555555555555556\n",
      "F1 weighted 0.39682539682539686\n"
     ]
    }
   ],
   "source": [
    "print(\"Weighted Accuracy\", accuracy_score(test_news['Label'], y_pred_weight1))\n",
    "print(\"F1 weighted\", f1_score(test_news['Label'], y_pred_weight1, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All scores are printed out for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Accuracy 0.5555555555555556\n",
      "F1 weighted 0.39682539682539686\n"
     ]
    }
   ],
   "source": [
    "# print(\"Sentiment Accuracy\", accuracy_score(test_news['Label'], y_pred))\n",
    "print(\"Weighted Accuracy\", accuracy_score(test_news['Label'], y_pred_weight1))\n",
    "print(\"F1 weighted\", f1_score(test_news['Label'], y_pred_weight1, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Headlines for the Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NYT():\n",
    "    your_key = 'XrEpgzLniLeuVv9Rwai6PZfol6OhEN91'\n",
    "\n",
    "    url = 'https://api.nytimes.com/svc/news/v3/content/all/all/title.json?api-key=' +your_key\n",
    "    r = requests.get(url)\n",
    "    json_data = r.json()\n",
    "    jdata = json_data['results']\n",
    "\n",
    "    daily_Headlines = []\n",
    "    NYT_Headlines = []\n",
    "\n",
    "    for x in jdata:\n",
    "#         for st in stocks:\n",
    "        if stock in x['title'].lower(): \n",
    "            oldformat = x['published_date']\n",
    "            date = oldformat.partition(\"T\")[0]\n",
    "            d = datetime.datetime.strptime(date, '%Y-%m-%d').date()\n",
    "            \n",
    "            NYT_Headlines.append(((stock, d , x['title'])))\n",
    "    df_2 = pd.DataFrame(NYT_Headlines, columns = [\"Ticker\", \"Date\", \"Headline\"])\n",
    "    return df_2\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(data):\n",
    "    output = []\n",
    "    for d in data:\n",
    "        date = datetime.strptime(d['publishedDate'], '%Y-%m-%d  %H:%M:%S').date()\n",
    "        output.append((d['symbol'],date,d['title']))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # For Python 3.0 and later\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    # Fall back to Python 2's urllib2\n",
    "    from urllib2 import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jsonparsed_data(url):\n",
    "    \"\"\"\n",
    "    Receive the content of ``url``, parse it as JSON and return the object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    \"\"\"\n",
    "    response = urlopen(url)\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FM(ticker):\n",
    "    key2 = \"f0448bd30a7028e245052fcf3caa0837\"\n",
    "    # ticker = ['AAPL','AMZN','MSFT','TSLA','TGT','WMT','DELL','F','BA']\n",
    "    f_output = []\n",
    "\n",
    " \n",
    "    url2 = (\"https://financialmodelingprep.com/api/v3/stock_news?tickers=\"+ticker+\"&apikey=\" + key2)\n",
    "\n",
    "\n",
    "    data = get_jsonparsed_data(url2)\n",
    "    output = parse_data(data)\n",
    "    df_1 = pd.DataFrame(output, columns = [\"Ticker\", \"Date\", \"Headline\"])\n",
    "    return df_1\n",
    "# df_2 = pd.DataFrame(NYT_Headlines, columns = [\"Ticker\", \"Date\", \"Headline\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2=get_NYT()\n",
    "df_1 = get_FM(ticker)\n",
    "# df_final = pd.append((df_1, df_2))\n",
    "df_final = pd.concat([df_1, df_2], ignore_index=True, sort=False)\n",
    "# df_final = df_1.append(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 Here's how Microsoft and Alphabet's earnings stack up\n",
      "2021-04-28 Alphabet, Aurora Cannabis, NXP Semiconductors and More Wednesday Afternoon Analyst Calls\n",
      "2021-04-28 Alphabet (GOOGL) Earnings Insights, Analysis, and Takeaways\n",
      "2021-04-28 Markets Await More Earnings Data\n",
      "2021-04-28 Traders Looking for Markets to Wake up\n",
      "2021-04-28 Alphabet (GOOGL) Earnings Crush Q1 Estimates, Revenues Rise Y/Y\n",
      "2021-04-28 Breaking down and gearing up for big tech earnings\n",
      "2021-04-28 Global stocks climb as investors look towards Fed decision, while US futures dip ahead of Apple and Facebook earnings (DJI, IXIC, NDX, SPX, AAPL, FB, GOOGL, TSLA, MSFT)\n",
      "2021-04-28 8 Stocks To Watch For April 28, 2021\n",
      "2021-04-28 Alphabet: 34% Revenue Growth And EBIT Doubling In Q1 2021\n"
     ]
    }
   ],
   "source": [
    "today = date.today()\n",
    "i = 0\n",
    "for x in df_final['Date']:\n",
    "    if today == x:\n",
    "        print(df_final['Date'][i], df_final['Headline'][i])\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_H = []\n",
    "i = 0\n",
    "num = 0\n",
    "headL_d = []\n",
    "for x in df_final['Date']:\n",
    "    if x == today:\n",
    "        if stock in df_final['Headline'][i].lower() or ticker.lower() in df_final['Headline'][i].lower() or (stock == \"google\" and \"alphabet\" in df_final['Headline'][i].lower()):\n",
    "            headL_d.append(df_final['Headline'][i].lower())\n",
    "            num = num + 1\n",
    "    i = i +1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_num = 0\n",
    "for x in news:\n",
    "    col_num = col_num + 1\n",
    "col_num = col_num -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   top1  top2  top3  top4  top5  top6  top7  top8  top9  top10  ...  top16  \\\n",
      "0  0.05   0.5  0.05  0.05  0.05  0.05     0     0     0      0  ...      0   \n",
      "\n",
      "   top17  top18  top19  top20  top21  top22  top23  top24  top25  \n",
      "0      0      0      0      0      0      0      0      0      0  \n",
      "\n",
      "[1 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "col2 = []\n",
    "for x in range(col_num):\n",
    "    col2.append(\"top\" + str(x + 1))\n",
    "    \n",
    "    if x < num  :\n",
    "        sent =analize_sentiment( headL_d[x])\n",
    "    else:\n",
    "        sent = int(0)\n",
    "\n",
    "    today_H.append(sent)\n",
    "\n",
    "last = []\n",
    "last.append(today_H)\n",
    "df_d = pd.DataFrame(last, columns = col2)\n",
    "print(df_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "0.5\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0.05\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for x in df_d:\n",
    "    for y in df_d[x]:\n",
    "        print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:18:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcard\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "all_train = copy.deepcopy(news)\n",
    "\n",
    "\n",
    "# all_train = all_train.drop(['Date', 'Label','1', '2', '3', '4', '5', '6', '7', '8', '9','10'], axis=1)\n",
    "all_train = all_train.drop(['Date', 'Label'], axis=1)\n",
    "for column in all_train:\n",
    "    all_train[column] = all_train[column].apply(analize_sentiment)\n",
    "# train_sentiment_weight = train_sentiment_weight \n",
    "\n",
    "\n",
    "weighted_All = XGBClassifier(scale_pos_weight = 60)\n",
    "weighted_All.fit(all_train, news['Label'])\n",
    "\n",
    "y_d2 = weighted_XGB1.predict(df_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for the day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_d = weighted_XGB1.predict(df_d)\n",
    "print(y_d2)\n",
    "print(y_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
